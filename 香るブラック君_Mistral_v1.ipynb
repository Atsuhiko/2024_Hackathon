{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8peXHyqqm5ge"
   },
   "source": [
    "# 香るブラック君_Mistral\n",
    "\n",
    "\n",
    "RTX-4090 実行: 2024/9/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 15 00:19:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 546.80       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090 L...    On | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   48C    P8                4W / 148W|    110MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/atsu/llamaindex\n",
      "/home/atsu/llamaindex/SadTalker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atsu/anaconda3/envs/llamaindex/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "%cd SadTalker\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Whisperのinstal --> 一度だけ実行\n",
    "# !pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid # 追加\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import gradio as gr\n",
    "import io\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import openai\n",
    "from huggingface_hub import login\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用した画像を削除してしまうので以下のコードをコメントアウトした。  \n",
    "shutil.move(source_image, input_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SadTalker用\n",
    "# from src.gradio_demo2 import SadTalker\n",
    "from inference6 import make_video # face_enhancer, background_enhancer 対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n"
     ]
    }
   ],
   "source": [
    "# VOICEVOX 用\n",
    "from pathlib import Path\n",
    "import voicevox_core\n",
    "from voicevox_core import AccelerationMode, AudioQuery, VoicevoxCore\n",
    "from playsound import playsound\n",
    "from pydub import AudioSegment\n",
    "# from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本パラメータ\n",
    "model_id = \"tokyotech-llm/Swallow-MS-7b-instruct-v0.1\"\n",
    "peft_name = \"../付喪神ジェネレータ/QLoRA_models/black_mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0ag7qhpfaZL"
   },
   "source": [
    "# Gradioを用いたWebアプリ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOICEVOX によるテキストから音声変換\n",
    "def VOICEVOX(text, out='output.wav', SPEAKER_ID=2):\n",
    "    open_jtalk_dict_dir = './open_jtalk_dic_utf_8-1.11'\n",
    "    acceleration_mode = AccelerationMode.AUTO\n",
    "    core = VoicevoxCore(\n",
    "        acceleration_mode=acceleration_mode, open_jtalk_dict_dir=open_jtalk_dict_dir\n",
    "    )\n",
    "    core.load_model(SPEAKER_ID)\n",
    "    audio_query = core.audio_query(text, SPEAKER_ID)\n",
    "    wav = core.synthesis(audio_query, SPEAKER_ID)\n",
    "    out_byte = Path(out) # 追加\n",
    "    out_byte.write_bytes(wav)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音源の速度を変える関数\n",
    "def sound_speed_change(sound_path, save_path, ratio):\n",
    "    # 曲の読み込み\n",
    "    sound = AudioSegment.from_wav(sound_path)\n",
    "    # 曲のスピードを変更\n",
    "    sound = sound.speedup(playback_speed=ratio, crossfade=0)\n",
    "    # 曲を保存する\n",
    "    sound.export(save_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a030fd512ada46098ec14e2f05b5f534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# モデルとトークナイザの準備\n",
    "# 量子化設定\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "# モデルの設定\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    # token=token, # HuggingFaceにログインしておけば不要\n",
    "    quantization_config=bnb_config, # 量子化\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "# tokenizerの設定\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True\n",
    ")\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.39 s, sys: 576 ms, total: 2.97 s\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ファインチューニングモデルの作成\n",
    "model = PeftModel.from_pretrained(base_model, peft_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章生成関数（EZO用）--> Llama3.1でもこのコードは有効だと分かった\n",
    "def generate(prompt, max_tokens=256, temperature=1, top_p=0.9, top_k=30):\n",
    "\n",
    "    prompt = f\"\"\"system prompt:あなたは缶コーヒーを飲むユーザーと対話するAIです。大阪弁で対話してください。短く返答してください。\n",
    "    user prompt: {prompt}\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=torch.ones(input_ids.shape, dtype=torch.long).to(model.device),\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response_text = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_response(input_mode, input_text, input_audio, image, voice, size, preprocess, face_enhancer, bkgnd_enhancer, \n",
    "                     max_tokens, temperature, top_p, top_k):\n",
    "    \n",
    "    # GPUメモリをリセット\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if input_mode == \"Text\":\n",
    "        input_text = input_text\n",
    "    else:\n",
    "        input_text = speech_to_text_GitHub(input_audio)\n",
    "\n",
    "    # GPUメモリをリセット\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 応答\n",
    "    code_regex = re.compile('[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠！｀ 　＋￥％abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789]')\n",
    "    response_text = code_regex.sub('', generate(input_text, max_tokens, temperature, top_p, top_k)).replace(\"\\n\",\"\")\n",
    "        # print(rag_result)\n",
    "    print(\"origilal response:\", response_text) \n",
    "\n",
    "    # 文末の「しゅ」に「。」が入らない場合の修正\n",
    "    response_text = response_text.replace(\"ですが\",\"やけど\").replace(\"ありますが\",\"あるんやけど\").replace(\"あり。\",\"ありますねん。\").\\\n",
    "                                  replace(\"しているのやねん。\",\"してるんねん。\").replace(\"わい、思いますねん。\",\"\").replace(\"わい\", \"俺\")\n",
    "\n",
    "    candidates = [\"で、どう思う？\", \"そうなんや、ええな\", \"ほんま？\", \"まじで？\", \"オチは？\"]\n",
    "    sample = random.choice(candidates)\n",
    "    response_text = response_text.replace(\"知らんけど\", sample)\n",
    "    \n",
    "    print(\"===============================\")\n",
    "    print(\"processed response text: \" + response_text)\n",
    "\n",
    "    # GPUメモリをリセット\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 返答を音声に\n",
    "    out = \"../大阪弁Talker/results/output.wav\"\n",
    "    if voice==\"玄野武宏\":\n",
    "        response_audio = VOICEVOX(text=response_text, out=out, SPEAKER_ID=11) #玄野武宏\n",
    "    elif voice==\"白上虎太郎\":\n",
    "        response_audio = VOICEVOX(text=response_text, out=out, SPEAKER_ID=12) #白上虎太郎\n",
    "    else:\n",
    "        response_audio = VOICEVOX(text=response_text, out=out, SPEAKER_ID=13) #青山龍星\n",
    "    \n",
    "    print(\"Sound generation finished.\") # ターミナルに表示\n",
    "    print(response_audio)\n",
    "\n",
    "    # 画像パスを引数に変更\n",
    "    image_path = image\n",
    "    print(image_path)\n",
    "    \n",
    "    # 追加のデバッグメッセージ\n",
    "    print(f\"Selected image path: {image_path}\")\n",
    "    if not os.path.isfile(image_path):\n",
    "        raise ValueError(f\"Image file does not exist: {image_path}\")\n",
    "    else:\n",
    "        print(f\"Image file exists: {image_path}\")\n",
    "\n",
    "    result_dir = '../大阪弁Talker/results'\n",
    "    \n",
    "    # ファイルパスのデバッグメッセージを追加\n",
    "    print(f\"Pic path before test call: {os.path.join(result_dir, str(uuid.uuid4()), 'input', os.path.basename(image_path))}\")\n",
    "\n",
    "    responce_video = make_video(image_path=image_path,\n",
    "                                audio_path=response_audio,\n",
    "                                size=size,\n",
    "                                preprocess=preprocess,\n",
    "                                enhancer=face_enhancer,\n",
    "                                background_enhancer=bkgnd_enhancer,\n",
    "                                result_dir=result_dir,\n",
    "                                )\n",
    "    \n",
    "    print(\"Video generation finished.\") # ターミナルに表示\n",
    "    print(responce_video)\n",
    "\n",
    "    # GPUメモリをリセット\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # return rag_result, responce_video, voice, image_path\n",
    "    return responce_video, voice, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blockの定義\n",
    "# https://note.com/npaka/n/n2a5112208b8d\n",
    "\n",
    "# CSSを使用してボタンのスタイルをカスタマイズ\n",
    "css = \"\"\"\n",
    "#custom-button {\n",
    "    background: #ee7800; /* ボタンの背景色 */\n",
    "    color: white; /* ボタンのテキスト色 */\n",
    "    border: none;\n",
    "    text-align: center;\n",
    "    font-size: 20px;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=css) as app:\n",
    "\n",
    "    # Markdown\n",
    "    gr.Markdown(\"\"\"# 超未来の付喪神ジェネレータ　香るブラック君\"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=300):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            input_mode = gr.Radio([\"Text\",\"Voice\"], label=\"Input mode\", value=\"Text\")\n",
    "                            input_text = gr.Textbox(placeholder=\"Please write the input.\", label=\"input text\",)\n",
    "                            record = gr.Audio(sources=[\"microphone\"], label=\"Input audio\", type=\"filepath\")\n",
    "                            voice = gr.Dropdown([\"玄野武宏\", \"白上虎太郎\", \"青山龍星\"], label=\"voice\", value=\"玄野武宏\")\n",
    "                            clear = gr.ClearButton(components=[record])\n",
    "                        with gr.Column():\n",
    "                            image = gr.Image(type='filepath') #画像のpathを返す\n",
    "                            clear = gr.ClearButton(components=[image])\n",
    "            with gr.Row():\n",
    "                btn = gr.Button(\"Response\", elem_id=\"custom-button\") #ボタンにIDを指定\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):                        \n",
    "                    with gr.Row():\n",
    "                        gr.Markdown(\"LLM congig\")\n",
    "                        max_tokens = gr.Dropdown([32, 64, 128, 256, 512, 1024, 2048, 4096, 8192], label=\"max tokens\", value=64)\n",
    "                        temperature = gr.Slider(minimum=0.01, maximum=1, value=1, step=0.01, label=\"temperature\", interactive=True)\n",
    "                        top_p = gr.Slider(minimum=0, maximum=1, value=0.9, step=0.01, label=\"top_p\", interactive=True)\n",
    "                        top_k = gr.Slider(minimum=1, maximum=100, value=30, step=1, label=\"top_k\", interactive=True)\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"SadTalker congig\")\n",
    "                    with gr.Row():\n",
    "                        preprocess = gr.Dropdown([\"crop\", \"extcrop\", \"resize\", \"full\", \"extfull\"], label=\"preprocess\", value=\"extcrop\")\n",
    "                        size = gr.Dropdown([256, 512], label=\"image size\", value=256)\n",
    "                        face_enhancer = gr.Dropdown([None, \"gfpgan\", \"RestoreFormer\"], label=\"face enhancer\", value=None)\n",
    "                        bkgnd_enhancer = gr.Dropdown([None, \"realesrgan\"], label=\"background enhancer\", value=None)\n",
    "            with gr.Row():\n",
    "                # rag_result = gr.Text(label=\"RAG result\")\n",
    "                agent_voice = gr.Text(label=\"Voice of agent\")\n",
    "                agent_image = gr.Text(label=\"Image of agent\")                \n",
    "        with gr.Column(scale=1, min_width=300):\n",
    "            response_video = gr.Video(label=\"Response video\")\n",
    "            # イベントリスナー\n",
    "            btn.click(fn=overall_response,\n",
    "                      inputs=[input_mode, input_text, record, image, voice, \n",
    "                              size, preprocess, face_enhancer, bkgnd_enhancer, max_tokens, temperature, top_p, top_k],                   \n",
    "                      outputs=[response_video, agent_voice, agent_image]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://c6f4e83b942e168773.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c6f4e83b942e168773.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origilal response: わい、思いますねん。コーヒーの効果を最大限に引き出すために、空腹時に飲むのが効果的やねん。また、ブラックで飲むか、無脂肪牛乳を入れて砂糖を足すか、脂肪入り牛乳を入れて砂糖とクリームを足すかにも\n",
      "===============================\n",
      "processed response text: コーヒーの効果を最大限に引き出すために、空腹時に飲むのが効果的やねん。また、ブラックで飲むか、無脂肪牛乳を入れて砂糖を足すか、脂肪入り牛乳を入れて砂糖とクリームを足すかにも\n",
      "Sound generation finished.\n",
      "../大阪弁Talker/results/output.wav\n",
      "/tmp/gradio/cc6231c6af6b55b93fa69750904c60ec0ebcb0bd/B02.png\n",
      "Selected image path: /tmp/gradio/cc6231c6af6b55b93fa69750904c60ec0ebcb0bd/B02.png\n",
      "Image file exists: /tmp/gradio/cc6231c6af6b55b93fa69750904c60ec0ebcb0bd/B02.png\n",
      "Pic path before test call: ../大阪弁Talker/results/f21108f8-9543-450c-a82c-3b350263a724/input/B02.png\n",
      "using safetensor as default\n",
      "3DMM Extraction for source image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "landmark Det:: 100%|██████████████████████████████████████| 1/1 [00:00<00:00, 10.48it/s]\n",
      "3DMM Extraction In Video:: 100%|██████████████████████████| 1/1 [00:00<00:00, 22.45it/s]\n",
      "mel:: 100%|████████████████████████████████████████| 317/317 [00:00<00:00, 61623.77it/s]\n",
      "audio2exp:: 100%|██████████████████████████████████████| 32/32 [00:00<00:00, 220.33it/s]\n",
      "Face Renderer:: 100%|█████████████████████████████████| 159/159 [00:18<00:00,  8.57it/s]\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (256, 255) to (256, 256) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated video is named ../大阪弁Talker/results/2024_09_15_00.20.30/B02##output.mp4\n",
      "Video generation finished.\n",
      "../大阪弁Talker/results/2024_09_15_00.20.30.mp4\n"
     ]
    }
   ],
   "source": [
    "# Webアプリを起動\n",
    "app.launch(share=True, debug=True) # share=Trueで一時的に公開される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNz/sMhizLOoO3w2ZAVjkv7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
