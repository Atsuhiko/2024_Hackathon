{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8peXHyqqm5ge"
   },
   "source": [
    "# 香るブラック君_Mistral\n",
    "\n",
    "\n",
    "RTX-4090 実行: 2024/9/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 15 08:14:34 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 546.80       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090 L...    On | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P8                5W /  55W|    148MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/atsu/llamaindex\n",
      "/home/atsu/llamaindex/SadTalker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atsu/anaconda3/envs/llamaindex/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "%cd SadTalker\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Whisperのinstal --> 一度だけ実行\n",
    "# !pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import uuid # 追加\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import gradio as gr\n",
    "import io\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import openai\n",
    "from huggingface_hub import login\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用した画像を削除してしまうので以下のコードをコメントアウトした。  \n",
    "shutil.move(source_image, input_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SadTalker用\n",
    "# from src.gradio_demo2 import SadTalker\n",
    "from inference6 import make_video # face_enhancer, background_enhancer 対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n"
     ]
    }
   ],
   "source": [
    "# VOICEVOX 用\n",
    "from pathlib import Path\n",
    "import voicevox_core\n",
    "from voicevox_core import AccelerationMode, AudioQuery, VoicevoxCore\n",
    "from playsound import playsound\n",
    "from pydub import AudioSegment\n",
    "# from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本パラメータ\n",
    "model_id = \"tokyotech-llm/Swallow-MS-7b-instruct-v0.1\"\n",
    "# ブラック君\n",
    "peft_name_black = \"../付喪神ジェネレータ/QLoRA_models/black_mistral\"\n",
    "# 紅茶さん\n",
    "peft_name_tea = \"../付喪神ジェネレータ/QLoRA_models/tea_mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API ログイン\n",
    "from openai import OpenAI\n",
    "with open('../API_key/OpenAI_API_key.txt', 'r') as f:\n",
    "    key = f.read()\n",
    "os.environ[\"OPENAI_API_KEY\"] = key\n",
    "openai.api_key = key\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0ag7qhpfaZL"
   },
   "source": [
    "# Gradioを用いたWebアプリ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声からテキストへの変換関数（新バージョン）\n",
    "# https://platform.openai.com/docs/guides/speech-to-text/quickstart\n",
    "def speech_to_text_API(input_audio):\n",
    "    audio_file = open(input_audio, \"rb\")\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\", \n",
    "        file=audio_file,\n",
    "    )\n",
    "    return transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOICEVOX によるテキストから音声変換\n",
    "def VOICEVOX(text, out='output.wav', SPEAKER_ID=2):\n",
    "    open_jtalk_dict_dir = './open_jtalk_dic_utf_8-1.11'\n",
    "    acceleration_mode = AccelerationMode.AUTO\n",
    "    core = VoicevoxCore(\n",
    "        acceleration_mode=acceleration_mode, open_jtalk_dict_dir=open_jtalk_dict_dir\n",
    "    )\n",
    "    core.load_model(SPEAKER_ID)\n",
    "    audio_query = core.audio_query(text, SPEAKER_ID)\n",
    "    wav = core.synthesis(audio_query, SPEAKER_ID)\n",
    "    out_byte = Path(out) # 追加\n",
    "    out_byte.write_bytes(wav)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a84edd3bdd6484e86ffd36da8595886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# モデルとトークナイザの準備\n",
    "# 量子化設定\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "# モデルの設定\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    # token=token, # HuggingFaceにログインしておけば不要\n",
    "    quantization_config=bnb_config, # 量子化\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "# tokenizerの設定\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True\n",
    ")\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.36 s, sys: 307 ms, total: 6.67 s\n",
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ファインチューニングモデルの作成\n",
    "base_model_2 = deepcopy(base_model)\n",
    "model_black = PeftModel.from_pretrained(base_model, peft_name_black)\n",
    "model_tea = PeftModel.from_pretrained(base_model_2, peft_name_tea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章生成関数: black君\n",
    "def generate_black(prompt, max_tokens=256, temperature=1, top_p=0.9, top_k=30):\n",
    "\n",
    "    prompt = f\"\"\"system prompt:あなたは缶コーヒーを飲むユーザーと対話するAIです。大阪弁で対話してください。短く返答してください。\n",
    "    user prompt: {prompt}\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model_black.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model_black.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=torch.ones(input_ids.shape, dtype=torch.long).to(model_black.device),\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response_text = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章生成関数: teaさん\n",
    "def generate_tea(prompt, max_tokens=256, temperature=1, top_p=0.9, top_k=30):\n",
    "\n",
    "    prompt = f\"\"\"system prompt:あなたは午後の紅茶を飲むユーザーと対話するAIです。お嬢さま口調で対話してください。短く返答してください。\n",
    "    user prompt: {prompt}\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model_tea.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model_tea.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=torch.ones(input_ids.shape, dtype=torch.long).to(model_tea.device),\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response_text = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_response(input_mode, input_text, input_audio, max_tokens):\n",
    "    \n",
    "    # GPUメモリをリセット\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if input_mode == \"Text\":\n",
    "        input_text = input_text\n",
    "    else:\n",
    "        input_text = speech_to_text_API(input_audio)\n",
    "\n",
    "    # GPUメモリをリセット\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 応答\n",
    "    code_regex = re.compile('[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠！｀ 　＋￥％？ξﾟ⊿abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789]')\n",
    "\n",
    "    seed = random.randint(0,1)\n",
    "    print(\"===============================\")\n",
    "    print(seed)\n",
    "    if seed==0:\n",
    "        # black君\n",
    "        response_text_black = code_regex.sub('', generate_black(input_text, max_tokens)).replace(\"\\n\",\"\")\n",
    "        response_text_black = response_text_black.replace(\"ですが\",\"やけど\").replace(\"ありますが\",\"あるんやけど\").replace(\"あり。\",\"ありますねん。\").\\\n",
    "                                      replace(\"しているのやねん。\",\"してるんねん。\").replace(\"わい、思いますねん。\",\"\").replace(\"わい\", \"俺\")\n",
    "        candidates = [\"で、どう思う？\", \"そうなんや、ええな\", \"ほんま？\", \"まじで？\", \"オチは？\"]\n",
    "        sample = random.choice(candidates)\n",
    "        response_text_black = response_text_black.replace(\"知らんけど\", sample)\n",
    "        # teaさん\n",
    "        response_text_tea = code_regex.sub('', generate_tea(response_text_black, max_tokens)).replace(\"\\n\",\"\")\n",
    "        print(\"response text black: \" + response_text_black)\n",
    "        print(\"response text tea: \" + response_text_tea)\n",
    "        print(\"===============================\")\n",
    "        \n",
    "    else:\n",
    "        # teaさん\n",
    "        response_text_tea = code_regex.sub('', generate_tea(input_text, max_tokens)).replace(\"\\n\",\"\")\n",
    "        # black君\n",
    "        response_text_black = code_regex.sub('', generate_black(response_text_tea, max_tokens)).replace(\"\\n\",\"\")\n",
    "        response_text_black = response_text_black.replace(\"ですが\",\"やけど\").replace(\"ありますが\",\"あるんやけど\").replace(\"あり。\",\"ありますねん。\").\\\n",
    "                                      replace(\"しているのやねん。\",\"してるんねん。\").replace(\"わい、思いますねん。\",\"\").replace(\"わい\", \"俺\")\n",
    "        candidates = [\"で、どう思う？\", \"そうなんや、ええな\", \"ほんま？\", \"まじで？\", \"オチは？\"]\n",
    "        sample = random.choice(candidates)\n",
    "        response_text_black = response_text_black.replace(\"知らんけど\", sample)\n",
    "        print(\"response text tea: \" + response_text_tea)\n",
    "        print(\"response text black: \" + response_text_black)\n",
    "        print(\"===============================\")\n",
    "        \n",
    "    # GPUメモリをリセット\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 返答を音声に\n",
    "    out_black = \"../付喪神ジェネレータ/results/output_black.wav\"\n",
    "    out_tea = \"../付喪神ジェネレータ/results/output_tea.wav\"\n",
    "    response_audio_black = VOICEVOX(text=response_text_black, out=out_black, SPEAKER_ID=13) #青山龍星\n",
    "    response_audio_tea = VOICEVOX(text=response_text_tea, out=out_tea, SPEAKER_ID=17) #九州そらセクシー\n",
    "    \n",
    "    print(\"Sound generation finished.\") # ターミナルに表示\n",
    "\n",
    "    # 画像パスを引数に変更\n",
    "    image_path_black = \"../付喪神ジェネレータ/agents/香るブラック君.png\"\n",
    "    image_path_tea = \"../付喪神ジェネレータ/agents/午後の紅茶さん.png\"\n",
    "\n",
    "    # 生成ビデオ保存フォルダ\n",
    "    result_dir = '../付喪神ジェネレータ/results'\n",
    "    \n",
    "    responce_video_black = make_video(image_path=image_path_black,\n",
    "                                audio_path=response_audio_black,\n",
    "                                size=256,\n",
    "                                preprocess=\"extcrop\",\n",
    "                                enhancer=None,\n",
    "                                background_enhancer=None,\n",
    "                                result_dir=result_dir,\n",
    "                                )\n",
    "\n",
    "    # GPUメモリをリセット\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    responce_video_tea = make_video(image_path=image_path_tea,\n",
    "                                audio_path=response_audio_tea,\n",
    "                                size=256,\n",
    "                                preprocess=\"extcrop\",\n",
    "                                enhancer=None,\n",
    "                                background_enhancer=None,\n",
    "                                result_dir=result_dir,\n",
    "                                )\n",
    "    \n",
    "    print(\"Video generation finished.\") # ターミナルに表示\n",
    "\n",
    "    # GPUメモリをリセット\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if seed==0:\n",
    "        responce_video_1 = responce_video_black\n",
    "        responce_video_2 = responce_video_tea\n",
    "    else:\n",
    "        responce_video_1 = responce_video_tea\n",
    "        responce_video_2 = responce_video_black\n",
    "        \n",
    "    print(seed)\n",
    "    print(\"responce_video_1\", responce_video_1)\n",
    "    print(\"responce_video_2\", responce_video_2)\n",
    "\n",
    "    return responce_video_1, responce_video_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blockの定義\n",
    "# https://note.com/npaka/n/n2a5112208b8d\n",
    "\n",
    "# CSSを使用してボタンのスタイルをカスタマイズ\n",
    "css = \"\"\"\n",
    "#custom-button {\n",
    "    background: #ee7800; /* ボタンの背景色 */\n",
    "    color: white; /* ボタンのテキスト色 */\n",
    "    border: none;\n",
    "    text-align: center;\n",
    "    font-size: 20px;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=css) as app:\n",
    "\n",
    "    # Markdown\n",
    "    gr.Markdown(\"\"\"# 超未来の付喪神ジェネレータ　香るブラック君 vs 午後のミルクティさん\"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1, min_width=300):\n",
    "            input_mode = gr.Radio([\"Text\",\"Voice\"], label=\"Input mode\", value=\"Text\")\n",
    "            input_text = gr.Textbox(placeholder=\"Please write the input.\", label=\"input text\",)\n",
    "            record = gr.Audio(sources=[\"microphone\"], label=\"Input audio\", type=\"filepath\")\n",
    "            clear = gr.ClearButton(components=[record])\n",
    "            max_tokens = gr.Dropdown([32, 64, 128, 256, 512, 1024, 2048, 4096, 8192], label=\"max tokens\", value=32)\n",
    "            btn = gr.Button(\"Response\", elem_id=\"custom-button\") #ボタンにIDを指定            \n",
    "        with gr.Column(scale=1, min_width=300):\n",
    "            response_video_1 = gr.Video()\n",
    "        with gr.Column(scale=1, min_width=300):\n",
    "            response_video_2 = gr.Video()\n",
    "            # イベントリスナー\n",
    "            btn.click(fn=overall_response,\n",
    "                      inputs=[input_mode, input_text, record, max_tokens],                   \n",
    "                      outputs=[response_video_1, response_video_2]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://3e86d95ff117f0fefb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3e86d95ff117f0fefb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "0\n",
      "response text black: コーヒーを淹れたいのやけど、どのように淹れるのがベストでしょうか\n",
      "response text tea: ワタクシ思いますの、午後のお茶にはいろいろな作り方がありますわー。コーヒーの種類にもよりますが\n",
      "===============================\n",
      "Sound generation finished.\n",
      "using safetensor as default\n",
      "3DMM Extraction for source image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "landmark Det:: 100%|██████████████████████████████████████| 1/1 [00:00<00:00,  9.43it/s]\n",
      "3DMM Extraction In Video:: 100%|██████████████████████████| 1/1 [00:00<00:00, 18.25it/s]\n",
      "mel:: 100%|████████████████████████████████████████| 102/102 [00:00<00:00, 97209.50it/s]\n",
      "audio2exp:: 100%|██████████████████████████████████████| 11/11 [00:00<00:00, 205.95it/s]\n",
      "Face Renderer:: 100%|███████████████████████████████████| 51/51 [00:31<00:00,  1.64it/s]\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (256, 241) to (256, 256) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated video is named ../付喪神ジェネレータ/results/2024_09_15_08.16.58/香るブラック君##output_black.mp4\n",
      "using safetensor as default\n",
      "3DMM Extraction for source image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "landmark Det:: 100%|██████████████████████████████████████| 1/1 [00:00<00:00, 26.39it/s]\n",
      "3DMM Extraction In Video:: 100%|██████████████████████████| 1/1 [00:00<00:00, 94.11it/s]\n",
      "mel:: 100%|████████████████████████████████████████| 251/251 [00:00<00:00, 18440.54it/s]\n",
      "audio2exp:: 100%|██████████████████████████████████████| 26/26 [00:00<00:00, 455.75it/s]\n",
      "Face Renderer:: 100%|█████████████████████████████████| 126/126 [01:18<00:00,  1.60it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated video is named ../付喪神ジェネレータ/results/2024_09_15_08.17.36/午後の紅茶さん##output_tea.mp4\n",
      "Video generation finished.\n",
      "0\n",
      "responce_video_1 ../付喪神ジェネレータ/results/2024_09_15_08.16.58.mp4\n",
      "responce_video_2 ../付喪神ジェネレータ/results/2024_09_15_08.17.36.mp4\n",
      "===============================\n",
      "1\n",
      "response text tea: ワタクシ思いますの、こんにちは午後の紅茶、おいしいでごわー。何かお手伝いしましょうか\n",
      "response text black: こんにちは午後の紅茶、美味しいですねさて、今日は何を\n",
      "===============================\n",
      "Sound generation finished.\n",
      "using safetensor as default\n",
      "3DMM Extraction for source image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "landmark Det:: 100%|██████████████████████████████████████| 1/1 [00:00<00:00,  4.93it/s]\n",
      "3DMM Extraction In Video:: 100%|██████████████████████████| 1/1 [00:00<00:00, 24.35it/s]\n",
      "mel:: 100%|███████████████████████████████████████| 106/106 [00:00<00:00, 101645.23it/s]\n",
      "audio2exp:: 100%|██████████████████████████████████████| 11/11 [00:00<00:00, 186.26it/s]\n",
      "Face Renderer:: 100%|███████████████████████████████████| 53/53 [00:56<00:00,  1.06s/it]\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (256, 241) to (256, 256) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated video is named ../付喪神ジェネレータ/results/2024_09_15_08.22.14/香るブラック君##output_black.mp4\n",
      "using safetensor as default\n",
      "3DMM Extraction for source image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "landmark Det:: 100%|██████████████████████████████████████| 1/1 [00:00<00:00, 26.52it/s]\n",
      "3DMM Extraction In Video:: 100%|██████████████████████████| 1/1 [00:00<00:00, 90.61it/s]\n",
      "mel:: 100%|███████████████████████████████████████| 270/270 [00:00<00:00, 107628.03it/s]\n",
      "audio2exp:: 100%|██████████████████████████████████████| 27/27 [00:00<00:00, 401.47it/s]\n",
      "Face Renderer:: 100%|█████████████████████████████████| 135/135 [01:24<00:00,  1.60it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated video is named ../付喪神ジェネレータ/results/2024_09_15_08.23.17/午後の紅茶さん##output_tea.mp4\n",
      "Video generation finished.\n",
      "1\n",
      "responce_video_1 ../付喪神ジェネレータ/results/2024_09_15_08.23.17.mp4\n",
      "responce_video_2 ../付喪神ジェネレータ/results/2024_09_15_08.22.14.mp4\n"
     ]
    }
   ],
   "source": [
    "# Webアプリを起動\n",
    "app.launch(share=True, debug=True) # share=Trueで一時的に公開される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNz/sMhizLOoO3w2ZAVjkv7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
